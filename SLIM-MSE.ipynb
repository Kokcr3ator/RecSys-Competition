{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from scipy.optimize import fmin\n",
    "import scipy.sparse as sps\n",
    "from Evaluation.Evaluator import EvaluatorHoldout\n",
    "from Data_manager.split_functions.split_train_validation_random_holdout import split_train_in_two_percentage_global_sample\n",
    "# Numpy cries because it is an old code so we monkey patch it\n",
    "np.int = int\n",
    "np.bool = bool\n",
    "np.float = float\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "URM_all_dataframe = pd.read_csv('data_train.csv', \n",
    "                                sep=\",\", \n",
    "                                header= 0, \n",
    "                                dtype={0:int, 1:int, 2:float},\n",
    "                                engine='python')\n",
    "\n",
    "URM_all_dataframe.columns = [\"UserID\", \"ItemID\", \"Interaction\"]\n",
    "\n",
    "\n",
    "# Build the COO sparse matrix associated with the URM\n",
    "URM_all = sps.coo_matrix((URM_all_dataframe[\"Interaction\"].values, \n",
    "                          (URM_all_dataframe[\"UserID\"].values, URM_all_dataframe[\"ItemID\"].values))) ## .values --> numpy array, df[..] --> pd series\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the dataset 80/20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: 593 (4.55 %) of 13025 users have no train items\n",
      "Warning: 2573 (19.75 %) of 13025 users have no sampled items\n",
      "Warning: 861 (6.61 %) of 13025 users have no train items\n",
      "Warning: 2913 (22.36 %) of 13025 users have no sampled items\n",
      "EvaluatorHoldout: Ignoring 2913 (22.4%) Users that have less than 1 test interactions\n",
      "EvaluatorHoldout: Ignoring 2573 (19.8%) Users that have less than 1 test interactions\n"
     ]
    }
   ],
   "source": [
    "URM_train, URM_test = split_train_in_two_percentage_global_sample(URM_all, train_percentage = 0.8)\n",
    "URM_train, URM_validation = split_train_in_two_percentage_global_sample(URM_train, train_percentage = 0.80)\n",
    "evaluator_validation = EvaluatorHoldout(URM_validation, cutoff_list=[10])\n",
    "evaluator_test = EvaluatorHoldout(URM_test, cutoff_list=[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build SLIMElasticNetRecommender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P3alphaRecommender: URM Detected 861 ( 6.6%) users with no interactions.\n",
      "P3alphaRecommender: URM Detected 455 ( 2.0%) items with no interactions.\n"
     ]
    }
   ],
   "source": [
    "from Recommenders.GraphBased.P3alphaRecommender import P3alphaRecommender\n",
    "recommender = P3alphaRecommender(URM_train, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content of stdout:\n",
      "_cython_magic_7dfaf4401a5ee9adcf9cfcc073561b3c96f72579.c\n",
      "   Creazione della libreria C:\\Users\\Tommaso\\.ipython\\cython\\Users\\Tommaso\\.ipython\\cython\\_cython_magic_7dfaf4401a5ee9adcf9cfcc073561b3c96f72579.cp311-win_amd64.lib e dell'oggetto C:\\Users\\Tommaso\\.ipython\\cython\\Users\\Tommaso\\.ipython\\cython\\_cython_magic_7dfaf4401a5ee9adcf9cfcc073561b3c96f72579.cp311-win_amd64.exp\n",
      "Generazione codice in corso...\n",
      "Generazione codice terminata"
     ]
    }
   ],
   "source": [
    "%%cython\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "import scipy.sparse as sps\n",
    "\n",
    "from libc.stdlib cimport rand, srand, RAND_MAX\n",
    "\n",
    "def train_multiple_epochs(URM_train, learning_rate_input, regularization_2_input, n_epochs,evaluator_validation, recommender_object,SM):\n",
    "\n",
    "    URM_train_coo = URM_train.tocoo()\n",
    "    cdef int n_items = URM_train.shape[1]\n",
    "    cdef int n_interactions = URM_train.nnz\n",
    "    cdef int[:] URM_train_coo_row = URM_train_coo.row\n",
    "    cdef int[:] URM_train_coo_col = URM_train_coo.col\n",
    "    cdef double[:] URM_train_coo_data = URM_train_coo.data\n",
    "    cdef int[:] URM_train_indices = URM_train.indices\n",
    "    cdef int[:] URM_train_indptr = URM_train.indptr\n",
    "    cdef double[:] URM_train_data = URM_train.data\n",
    "\n",
    "    cdef double[:,:] item_item_S = SM\n",
    "    cdef double learning_rate = learning_rate_input\n",
    "    cdef double regularization_2 = regularization_2_input\n",
    "    cdef double loss = 0.0\n",
    "    cdef long start_time\n",
    "    cdef double true_rating, predicted_rating, prediction_error, profile_rating\n",
    "    cdef int start_profile, end_profile\n",
    "    cdef int index, sample_num, user_id, item_id, profile_item_id\n",
    "    \n",
    "    for n_epoch in range(n_epochs):\n",
    "        \n",
    "        loss = 0.0\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for sample_num in range(n_interactions):\n",
    "\n",
    "            # Randomly pick sample\n",
    "            index = rand() % n_interactions\n",
    "\n",
    "            user_id = URM_train_coo_row[index]\n",
    "            item_id = URM_train_coo_col[index]\n",
    "            true_rating = URM_train_coo_data[index]\n",
    "\n",
    "            # Compute prediction\n",
    "            start_profile = URM_train_indptr[user_id]\n",
    "            end_profile = URM_train_indptr[user_id+1]\n",
    "            predicted_rating = 0.0\n",
    "\n",
    "            for index in range(start_profile, end_profile):\n",
    "                profile_item_id = URM_train_indices[index]\n",
    "                profile_rating = URM_train_data[index]\n",
    "                predicted_rating += item_item_S[profile_item_id,item_id] * profile_rating\n",
    "\n",
    "            # Compute prediction error, or gradient\n",
    "            prediction_error = true_rating - predicted_rating\n",
    "            loss += prediction_error**2\n",
    "\n",
    "            # Update model, in this case the similarity\n",
    "            for index in range(start_profile, end_profile):\n",
    "                profile_item_id = URM_train_indices[index]\n",
    "                profile_rating = URM_train_data[index]\n",
    "                item_item_S[profile_item_id,item_id] += learning_rate * (prediction_error * profile_rating - \n",
    "                                                                         regularization_2 * item_item_S[profile_item_id,item_id])\n",
    "\n",
    "            # Ensure diagonal is always zero\n",
    "            item_item_S[item_id,item_id] = 0.0\n",
    "        \n",
    "#             if sample_num % 1000000 == 0:\n",
    "#                 print(\"Epoch {}: {:.2f}%\".format(n_epoch+1, sample_num/n_interactions*100))\n",
    "            \n",
    "        #result_df, _ = evaluator_validation.evaluateRecommender(recommender_object)\n",
    "        #val_MAP = result_df.loc[10][\"MAP\"]\n",
    "        elapsed_time = time.time() - start_time\n",
    "        samples_per_second = (sample_num+1)/elapsed_time\n",
    "\n",
    "        if(n_epoch < 1000 ):\n",
    "            print(\"Epoch {} complete in in {:.2f} seconds, loss is {:.3E}. Samples per second {:.2f}.\".format(n_epoch+1, time.time() - start_time, loss/(sample_num+1), samples_per_second))\n",
    "        else:\n",
    "            sparse_similarity = sps.csr_matrix(item_item_S)\n",
    "            recommender_object.set_similarity_matrix(sparse_similarity)\n",
    "            result_df, _ = evaluator_validation.evaluateRecommender(recommender_object)\n",
    "            val_MAP = result_df.loc[10][\"MAP\"]\n",
    "            print(\"Epoch {} complete in in {:.2f} seconds, loss is {:.3E}. Samples per second {:.2f}. Validation MAP {:.4f}\".format(n_epoch+1, time.time() - start_time, loss/(sample_num+1), samples_per_second,val_MAP))\n",
    "\n",
    "    return np.array(item_item_S), loss/(sample_num+1), val_MAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "regularizations_array = np.logspace(-2.5, 1, num=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_matrix_list = []\n",
    "loss_list = []\n",
    "val_map_list = []\n",
    "learning_rate = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(regularizations_array)):\n",
    "    # Initializing the similarity matrix\n",
    "    n_items = URM_train.shape[1]\n",
    "    item_item_S = np.zeros((n_items, n_items), dtype = float)\n",
    "    item_item_S, loss, map = train_multiple_epochs(URM_train, learning_rate, regularizations_array[i],1001, evaluator_validation, recommender,item_item_S)\n",
    "    sim_matrix_list.append(item_item_S)\n",
    "    loss_list.append(loss)\n",
    "    val_map_list.append(map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting the Similarity matrix in a recommender_object instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.010403661497753383,\n",
       " 0.010081089892359028,\n",
       " 0.008689386834901055,\n",
       " 0.007378339506348257,\n",
       " 0.008551618877111238]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_map_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_item_S = np.load('W_sparse.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = item_item_S['indices']\n",
    "indptr = item_item_S['indptr']\n",
    "format = item_item_S['format']\n",
    "shape = item_item_S['shape']\n",
    "data = item_item_S['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_similarity = csr_matrix((data, indices, indptr), shape=shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommender.set_similarity_matrix(sparse_similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EvaluatorHoldout: Processed 10054 (100.0%) in 6.73 sec. Users per second: 1493\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PRECISION                      0.128218\n",
       "PRECISION_RECALL_MIN_DEN       0.264678\n",
       "RECALL                         0.241776\n",
       "MAP                            0.065091\n",
       "MAP_MIN_DEN                    0.133504\n",
       "MRR                            0.328312\n",
       "NDCG                            0.22052\n",
       "F1                              0.16757\n",
       "HIT_RATE                       0.638651\n",
       "ARHR_ALL_HITS                  0.455674\n",
       "NOVELTY                        0.005334\n",
       "AVERAGE_POPULARITY             0.177179\n",
       "DIVERSITY_MEAN_INTER_LIST      0.980729\n",
       "DIVERSITY_HERFINDAHL           0.998063\n",
       "COVERAGE_ITEM                  0.468409\n",
       "COVERAGE_ITEM_HIT              0.248568\n",
       "ITEMS_IN_GT                     0.73756\n",
       "COVERAGE_USER                    0.7719\n",
       "COVERAGE_USER_HIT              0.492975\n",
       "USERS_IN_GT                      0.7719\n",
       "DIVERSITY_GINI                  0.09824\n",
       "SHANNON_ENTROPY               10.774877\n",
       "RATIO_DIVERSITY_HERFINDAHL     0.998424\n",
       "RATIO_DIVERSITY_GINI           0.286529\n",
       "RATIO_SHANNON_ENTROPY          0.830663\n",
       "RATIO_AVERAGE_POPULARITY        1.49395\n",
       "RATIO_NOVELTY                  0.390637\n",
       "Name: 10, dtype: object"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df, _ = evaluator_validation.evaluateRecommender(recommender)\n",
    "result_df.loc[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PRECISION</th>\n",
       "      <th>PRECISION_RECALL_MIN_DEN</th>\n",
       "      <th>RECALL</th>\n",
       "      <th>MAP</th>\n",
       "      <th>MAP_MIN_DEN</th>\n",
       "      <th>MRR</th>\n",
       "      <th>NDCG</th>\n",
       "      <th>F1</th>\n",
       "      <th>HIT_RATE</th>\n",
       "      <th>ARHR_ALL_HITS</th>\n",
       "      <th>...</th>\n",
       "      <th>COVERAGE_USER</th>\n",
       "      <th>COVERAGE_USER_HIT</th>\n",
       "      <th>USERS_IN_GT</th>\n",
       "      <th>DIVERSITY_GINI</th>\n",
       "      <th>SHANNON_ENTROPY</th>\n",
       "      <th>RATIO_DIVERSITY_HERFINDAHL</th>\n",
       "      <th>RATIO_DIVERSITY_GINI</th>\n",
       "      <th>RATIO_SHANNON_ENTROPY</th>\n",
       "      <th>RATIO_AVERAGE_POPULARITY</th>\n",
       "      <th>RATIO_NOVELTY</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cutoff</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.128218</td>\n",
       "      <td>0.264678</td>\n",
       "      <td>0.241776</td>\n",
       "      <td>0.065091</td>\n",
       "      <td>0.133504</td>\n",
       "      <td>0.328312</td>\n",
       "      <td>0.22052</td>\n",
       "      <td>0.16757</td>\n",
       "      <td>0.638651</td>\n",
       "      <td>0.455674</td>\n",
       "      <td>...</td>\n",
       "      <td>0.7719</td>\n",
       "      <td>0.492975</td>\n",
       "      <td>0.7719</td>\n",
       "      <td>0.09824</td>\n",
       "      <td>10.774877</td>\n",
       "      <td>0.998424</td>\n",
       "      <td>0.286529</td>\n",
       "      <td>0.830663</td>\n",
       "      <td>1.49395</td>\n",
       "      <td>0.390637</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       PRECISION PRECISION_RECALL_MIN_DEN    RECALL       MAP MAP_MIN_DEN  \\\n",
       "cutoff                                                                      \n",
       "10      0.128218                 0.264678  0.241776  0.065091    0.133504   \n",
       "\n",
       "             MRR     NDCG       F1  HIT_RATE ARHR_ALL_HITS  ... COVERAGE_USER  \\\n",
       "cutoff                                                      ...                 \n",
       "10      0.328312  0.22052  0.16757  0.638651      0.455674  ...        0.7719   \n",
       "\n",
       "       COVERAGE_USER_HIT USERS_IN_GT DIVERSITY_GINI SHANNON_ENTROPY  \\\n",
       "cutoff                                                                \n",
       "10              0.492975      0.7719        0.09824       10.774877   \n",
       "\n",
       "       RATIO_DIVERSITY_HERFINDAHL RATIO_DIVERSITY_GINI RATIO_SHANNON_ENTROPY  \\\n",
       "cutoff                                                                         \n",
       "10                       0.998424             0.286529              0.830663   \n",
       "\n",
       "       RATIO_AVERAGE_POPULARITY RATIO_NOVELTY  \n",
       "cutoff                                         \n",
       "10                      1.49395      0.390637  \n",
       "\n",
       "[1 rows x 27 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(recommender.recommend(41)[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write the predictions on submission.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   user_id                                        item_list\n",
      "0        1            36 101 354 1 123 139 254 4034 146 318\n",
      "1        2                  11 50 47 1227 1095 28 102 2 3 1\n",
      "2        3            59 2 648 259 584 450 414 536 4252 399\n",
      "3        4               11 50 47 249 28 252 353 129 51 133\n",
      "4        5           3063 202 766 256 270 192 301 471 4 447\n",
      "5        6                35 9 692 88 14 168 56 886 104 395\n",
      "6        8          210 682 451 778 760 366 443 480 722 600\n",
      "7        9  2821 2282 8743 341 9018 1460 227 1366 2508 1206\n",
      "8       10     2145 1816 1618 561 1411 3721 4267 31 3905 67\n",
      "9       11                   31 67 39 40 99 955 58 93 32 34\n"
     ]
    }
   ],
   "source": [
    "def write_predictions(recommender_object, at=10):\n",
    "    prediction_df = pd.read_csv('data_target_users_test.csv', sep= \",\",\n",
    "                                header=0, \n",
    "                                dtype={0:int},\n",
    "                                engine='python')\n",
    "    #prediction_df['mapped user'] = prediction_df['user_id'].map(user_original_ID_to_index).fillna(-1).astype(int)\n",
    "    #prediction_df['item_list'] = prediction_df['mapped user'].apply(recommender_object.recommend)\n",
    "    prediction_df['item_list'] = prediction_df['user_id'].apply(lambda user_id: recommender_object.recommend(user_id)[:at])\n",
    "    #def map_items(item_list):\n",
    "    #    return [index_to_item_original.get(item, item) for item in item_list]\n",
    "\n",
    "    #prediction_df['item_list'] = prediction_df['item_list'].apply(map_items)\n",
    "    def transform_items_to_string(item_list):\n",
    "        return ' '.join(map(str, item_list))\n",
    "\n",
    "    prediction_df['item_list'] = prediction_df['item_list'].apply(transform_items_to_string)\n",
    "    print(prediction_df.head(10))\n",
    "    #del prediction_df['mapped user']\n",
    "    prediction_df.to_csv('submission.csv',index=False)\n",
    "\n",
    "write_predictions(recommender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_df = pd.read_csv('submission.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
